{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the main results we obtained with Deep Reinforcement Learning on the three\n",
    "tractable cost models considered in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# local imports\n",
    "from agent import Agent\n",
    "from env import Environment\n",
    "from evaluation import test_models, plot_bars, plot_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear trading costs with risk penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "c(\\pi_t, p_t) = -\\pi_t p_t + \\lambda \\pi_t^2 + \\psi | \\pi_t - \\pi_{t-1}|;\\hspace{2mm}\n",
    "\\lambda, \\psi > 0\n",
    "$$\n",
    "\n",
    "***Approximate Optimal Solution***\n",
    "\n",
    "for simplicity we will consider an approximate optimal solution which shape is\n",
    "parameterized making this solution easily found by a simple girdSearch.\n",
    "\n",
    "$$ f\\left(\\pi_{t-1}, p_t \\right) = \\begin{cases} \\frac{1}{2\\widetilde{\\lambda}}\\left(p_t\n",
    "- \\widetilde{\\psi} \\right) - \\pi_{t-1}; \\hspace{2mm} p_t \\ge \\widetilde{\\psi} +\n",
    "2\\widetilde{\\lambda} \\pi_{t-1} \\\\ 0 \\hspace{13mm}; \\hspace{2mm} -\\widetilde{\\psi} +\n",
    "2\\widetilde{\\lambda} \\pi_{t-1} \\le p_t \\le \\widetilde{\\psi} + 2\\widetilde{\\lambda}\n",
    "\\pi_{t-1} \\\\ \\frac{1}{2\\widetilde{\\lambda}}\\left( p_t + \\widetilde{\\psi} \\right)-\n",
    "\\pi_{t-1}; \\hspace{2mm} p_t \\le -\\widetilde{\\psi} + 2\\widetilde{\\lambda} \\pi_{t-1}\n",
    "\\end{cases} $$\n",
    "\n",
    "Parameters $\\widetilde{\\lambda}, \\widetilde{\\psi}$ can be found with a gridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Environment***\n",
    "\n",
    "We set our environment with the follwing parameters:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\theta = 0.1 \\\\\n",
    "\\sigma = 0.1 \\\\\n",
    "T = 5000 \\\\\n",
    "\\lambda = 0.3 \\\\\n",
    "\\psi = 4\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "\n",
    "SIGMA = 0.1\n",
    "THETA = 0.1\n",
    "T = 5000\n",
    "LAMBD = 0.3\n",
    "PSI = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(sigma=SIGMA, theta=THETA, T=T, lambd=LAMBD,\n",
    "                  psi=PSI, cost='trade_l1', scale_reward=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***GridSearch***\n",
    "\n",
    "We perform a GridSearch to look for the optimal parameters $\\widetilde{\\lambda},\n",
    "\\widetilde{\\psi}$ using the average cumulative reward over $10$ episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random_state = 1024\n",
    "n_episodes = 10\n",
    "rng = np.random.RandomState(random_state)\n",
    "random_states = rng.randint(0, int(1e6), size=n_episodes)\n",
    "\n",
    "lambds = np.linspace(0.2, 0.6, 10)\n",
    "psis = np.linspace(0.8, 1.2, 10)\n",
    "\n",
    "# 1st dim: lambdas; 2nd dim: psis; 3rd dim: episodes\n",
    "scores_episodes = np.empty((len(lambds), len(psis), n_episodes))\n",
    "scores = np.empty((len(lambds), len(psis)))  # 1st dim: lambdas; 2nd dim: psis\n",
    "\n",
    "for i, lambd in tqdm_notebook(list(enumerate(lambds))):\n",
    "    for j, psi in tqdm_notebook(list(enumerate(psis))):\n",
    "        score, score_episode, _, _, _ = env.test_apply(\n",
    "            total_episodes=n_episodes, random_states=random_states, lambd=lambd, psi=psi)\n",
    "        scores[i, j] = score\n",
    "        scores_episodes[i, j, :] = list(score_episode.values())\n",
    "        print('lambd=%.1f , psi=%.1f -> score=%.3f \\n' % (lambd, psi, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pd.DataFrame(\n",
    "    scores,\n",
    "    index=pd.Index(np.round(lambds, 2), name=r'$\\widetilde{\\lambda}$'),\n",
    "    columns=pd.Index(np.round(psis, 2), name=r'$\\widetilde{\\psi}$')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max = np.argmax(scores)//scores.shape[0]\n",
    "j_max = np.argmax(scores[i_max, :])\n",
    "\n",
    "lambd_max, psi_max = lambds[i_max], psis[j_max]\n",
    "print('lambd_max=%.2f , psi_max=%.2f' % (lambd_max, psi_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that $\\widetilde{\\lambda}=0.47, \\widetilde{\\psi}=0.93$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is an actor critic architecture, both actor and critic networks are composed\n",
    "of an inital fully connected layer of size $16$ with a reLU activation, a hidden fully\n",
    "connected layer of size $16$ with a reLU activation and a linear output layer of size\n",
    "$1$. The agent explores the environment with its current policy, an additive OU noise of\n",
    "parameters $\\theta=1, \\sigma=1$, it puts the experiences in a replay buffer of size\n",
    "$10^6$ and, we use Prioritized Experience Replay to sample a batch of size $512$ each\n",
    "$50$ time steps when we run a learning iteration. Before the training process starts, we\n",
    "run a pretraining phase and fill the replay buffer with $1000$ experiences generated by\n",
    "exploring the environment with the initial Actor network plus the additional OU noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent parameters\n",
    "MAX_STEPS = 5000\n",
    "MEMORY_TYPE = 'prioritized'\n",
    "SLIDING = 'oldest'\n",
    "BATCH_SIZE = 2**9\n",
    "MAX_SIZE = int(1e6)\n",
    "\n",
    "# Training parameters\n",
    "TOTAL_EPISODES = 200  # set it to 501 for better convergence\n",
    "TOTAL_STEPS = 1000\n",
    "FREQ = 10\n",
    "LEARN_FREQ = 50\n",
    "TAU_ACTOR = 0.3\n",
    "TAU_CRITIC = 0.1\n",
    "LR_ACTOR = 1e-3\n",
    "LR_CRITIC = 1e-2\n",
    "WEIGHTS_DECAY_ACTOR = 0\n",
    "WEIGHTS_DECAY_CRITIC = 0\n",
    "FC1_UNITS_ACTOR = 16\n",
    "FC2_UNITS_ACTOR = 16\n",
    "FC1_UNITS_CRITIC = 16\n",
    "FC2_UNITS_CRITIC = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agent = Agent(max_size=MAX_SIZE, max_step=MAX_STEPS, memory_type=MEMORY_TYPE,\n",
    "              sliding=SLIDING, batch_size=BATCH_SIZE)\n",
    "\n",
    "path = 'Experiment_linear_trading_cost_true_per/'\n",
    "if not os.path.exists(path + 'weights/'):\n",
    "    os.makedirs(path + 'weights/')\n",
    "\n",
    "agent.train(env=env, total_episodes=TOTAL_EPISODES, tau_actor=TAU_ACTOR,\n",
    "            tau_critic=TAU_CRITIC, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC,\n",
    "            weight_decay_actor=WEIGHTS_DECAY_ACTOR,\n",
    "            weight_decay_critic=WEIGHTS_DECAY_CRITIC, total_steps=TOTAL_STEPS,\n",
    "            weights=path+'weights/', freq=FREQ, fc1_units_actor=FC1_UNITS_ACTOR,\n",
    "            fc2_units_actor=FC2_UNITS_ACTOR, fc1_units_critic=FC1_UNITS_CRITIC,\n",
    "            fc2_units_critic=FC2_UNITS_CRITIC, learn_freq=LEARN_FREQ, plots=True,\n",
    "            lambd=lambd_max, psi=psi_max, tensordir=path+'runs/', mile=100,\n",
    "            decay_rate=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluation***\n",
    "\n",
    "We evaluate our models and compare them with the approximate optimal solution on $10$\n",
    "new test episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_weights = path+'weights/'\n",
    "scores, scores_episodes, scores_cumsum, pnls, positions = test_models(\n",
    "    path_weights, env, n_episodes=10, fc1_units=16, fc2_units=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1024\n",
    "n_episodes = 10\n",
    "rng = np.random.RandomState(random_state)\n",
    "random_states = rng.randint(0, int(1e6), size=n_episodes)\n",
    "score, score_episode, scores_cumsum_opt, pnls_opt, positions_opt = env.test_apply(\n",
    "    total_episodes=n_episodes, random_states=random_states, lambd=lambd_max, psi=psi_max)\n",
    "scores[-1] = score\n",
    "scores_episodes[-1] = score_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a predictor signal, we visualize both the positions taken by our agent and those\n",
    "taken by the approximate optimal solution, we find that the positions evolve very\n",
    "similarly ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(random_state=730001)\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(positions[110][730001], label='DDPG', color='g')\n",
    "plt.plot(positions_opt[730001], label='OPT', color='r')\n",
    "plt.plot(env.signal[1:], label='signal$', color='y')\n",
    "plt.xlim(300, 600)\n",
    "plt.xlabel(r'$t$', fontsize=15)\n",
    "plt.ylabel(r'$p_t, \\pi_t$', fontsize=15)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(positions[110][730001], label='DDPG', color='g')\n",
    "plt.plot(positions_opt[730001], label='OPT', color='r')\n",
    "plt.xlim(300, 600)\n",
    "plt.xlabel(r'$t$', fontsize=15)\n",
    "plt.ylabel(r'$\\pi_t$', fontsize=15)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('positions_penalty.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = sorted(scores.values(), reverse=True)\n",
    "print('Optimal agent score   : %.2f' % sorted_scores[0])\n",
    "print('Best DDPG agent score : %.2f' % sorted_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_weights = path+'weights/'\n",
    "models_keys = [0, 10, 50, 90, 110]\n",
    "plot_function(path_weights, env, models_keys, low=-4, high=4,\n",
    "              lambd=lambd_max, psi=psi_max, fc1_units=16, fc2_units=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear trading costs with risk constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can model the problem in two ways here:\n",
    "\n",
    "We keep the reward: $r(\\pi_t, p_t) = \\pi_t p_t - \\psi | \\pi_t - \\pi_{t-1}|;\\hspace{2mm}\n",
    "\\lambda, \\psi > 0$ and clip actions in the interval $\\left[-M, M\\right]$; this means\n",
    "that an agent does not get penalized for making action $a$ taking its position $\\pi$\n",
    "beyond $M$, **i.e** $|\\pi + a|>M$, since its next position $\\pi'$ is s.t $|\\pi'|=M$.\n",
    "\n",
    "The second way is harder, it does not involve clipping the positions but rather\n",
    "penalizes the agent for making actions taking its position outside the interval\n",
    "$\\left[-M, M \\right]$. We can do this by adding a smooth penalty to the reward, and we\n",
    "choose a $tanh$ barrier in the following way:\n",
    "$$ r(\\pi_t, p_t) = \\pi_t p_t - \\psi | \\pi_t - \\pi_{t-1}| - \\beta\\left( tanh\\left[\n",
    "\\alpha\\left( |\\pi_t| - M - \\gamma\\right)\\right] + 1\\right);\\hspace{2mm} \\lambda, \\psi,\n",
    "\\alpha, \\beta, \\gamma > 0 $$\n",
    "This $tanh$ penalty is smooth and does not diverge for high $|\\pi|$ values which makes\n",
    "it useful in training stabilization compared to a constant penalty or an exponential\n",
    "one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optimal Solution***\n",
    "\n",
    "for simplicity we will consider an approximate optimal solution which shape is\n",
    "parameterized making this solution easily found by a simple grid-search.\n",
    "\n",
    "$$\n",
    "f\\left(\\pi_{t-1}, p_t \\right) =\n",
    "\\begin{cases}\n",
    "M - \\pi_{t-1}; \\hspace{2mm} p_t > \\widetilde{q} \\\\\n",
    "0 \\hspace{13mm}; \\hspace{2mm} |p_t| \\le \\widetilde{q} \\\\\n",
    "-M - \\pi_{t-1}; \\hspace{2mm} p_t < -\\widetilde{q}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Parameters $\\widetilde{q}$ can be found with a gridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Environment***\n",
    "\n",
    "First, we will work with the first setting and only use an experience generator to get\n",
    "some insight about how to properly explore and learn. Then we will work on both settings\n",
    "in the exploration-learning scheme training.\n",
    "\n",
    "We set our environment with the follwing parameters:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\theta = 0.1 \\\\\n",
    "\\sigma = 0.1 \\\\\n",
    "T = 5000 \\\\\n",
    "M = 2 \\\\\n",
    "\\psi = 4\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "\n",
    "SIGMA = 0.1\n",
    "THETA = 0.1\n",
    "T = 5000\n",
    "MAXPOS = 2\n",
    "PSI = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(sigma=SIGMA, theta=THETA, T=T, psi=PSI, cost='trade_l1', squared_risk=False,\n",
    "                  max_pos=MAXPOS, clip=True, penalty='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta, gamma = 10, 10, MAXPOS/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_penalty(pi): return beta*(np.tanh(alpha*(abs(pi) - MAXPOS - gamma)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 5), tanh_penalty(np.linspace(0, 5)), label='tanh_penalty')\n",
    "plt.axvline(MAXPOS, color='r', label='MAXPOS')\n",
    "plt.title('tanh smooth penalty barrier', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('tanh_barrier.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent parameters\n",
    "MAX_STEPS = 5000\n",
    "MEMORY_TYPE = 'prioritized'\n",
    "SLIDING = 'oldest'\n",
    "BATCH_SIZE = 2**9\n",
    "MAX_SIZE = int(1e6)\n",
    "\n",
    "# Training parameters\n",
    "TOTAL_EPISODES = 200  # set it to 501 for better convergence\n",
    "TOTAL_STEPS = 10000\n",
    "FREQ = 10\n",
    "LEARN_FREQ = 50\n",
    "TAU_ACTOR = 0.3\n",
    "TAU_CRITIC = 0.1\n",
    "LR_ACTOR = 1e-3\n",
    "LR_CRITIC = 1e-2\n",
    "WEIGHTS_DECAY_ACTOR = 0\n",
    "WEIGHTS_DECAY_CRITIC = 0\n",
    "FC1_UNITS_ACTOR = 16\n",
    "FC2_UNITS_ACTOR = 16\n",
    "FC1_UNITS_CRITIC = 16\n",
    "FC2_UNITS_CRITIC = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agent = Agent(max_size=MAX_SIZE, max_step=MAX_STEPS, memory_type=MEMORY_TYPE,\n",
    "              sliding=SLIDING, batch_size=BATCH_SIZE, alpha=0.6, theta=.1)\n",
    "\n",
    "path = 'Experiment_maxpos/'\n",
    "if not os.path.exists(path + 'weights/'):\n",
    "    os.makedirs(path+'weights/')\n",
    "\n",
    "agent.train(env=env, total_episodes=TOTAL_EPISODES, tau_actor=TAU_ACTOR,\n",
    "            tau_critic=TAU_CRITIC, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC,\n",
    "            weight_decay_actor=WEIGHTS_DECAY_ACTOR,\n",
    "            weight_decay_critic=WEIGHTS_DECAY_CRITIC, total_steps=TOTAL_STEPS,\n",
    "            weights=path+'weights/', freq=FREQ, fc1_units_actor=FC1_UNITS_ACTOR,\n",
    "            fc2_units_actor=FC2_UNITS_ACTOR, fc1_units_critic=FC1_UNITS_CRITIC,\n",
    "            fc2_units_critic=FC2_UNITS_CRITIC, learn_freq=LEARN_FREQ, plots=True,\n",
    "            thresh=0.95, tensordir=path+'runs/', mile=100, decay_rate=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_weights = path+'weights/'\n",
    "scores, scores_episodes, scores_cumsum, pnls, positions = test_models(\n",
    "    path_weights, env, n_episodes=10, fc1_units=16, fc2_units=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1024\n",
    "n_episodes = 10\n",
    "rng = np.random.RandomState(random_state)\n",
    "random_states = rng.randint(0, int(1e6), size=n_episodes)\n",
    "score, score_episode, scores_cumsum_opt, pnls_opt, positions_opt = env.test_apply(\n",
    "    total_episodes=n_episodes, random_states=random_states, thresh=.95)\n",
    "scores[-1] = score\n",
    "scores_episodes[-1] = score_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_keys = [0, 20, 80, 120, 140]\n",
    "plot_function(path_weights, env, models_keys, low=-4, high=4, lambd=0.3,\n",
    "              fc1_units=16, fc2_units=16, thresh=.95, clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(random_state=989115)\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(positions[140][989115], label='DDPG', color='g')\n",
    "plt.plot(positions_opt[989115], label='OPT', color='r')\n",
    "plt.plot(env.signal[1:], label='signal$', color='y')\n",
    "plt.xlim(0, 300)\n",
    "plt.xlabel(r'$t$', fontsize=15)\n",
    "plt.ylabel(r'$p_t, \\pi_t$', fontsize=15)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(positions[140][989115], label='DDPG', color='g')\n",
    "plt.plot(positions_opt[989115], label='OPT', color='r')\n",
    "plt.xlim(0, 300)\n",
    "plt.ylim(-4, 4)\n",
    "plt.xlabel(r'$t$', fontsize=15)\n",
    "plt.ylabel(r'$\\pi_t$', fontsize=15)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('positions_constraint_per.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squared impact model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the following cost model incorporating a squared impact cost:\n",
    "\n",
    "$$\n",
    "c(\\pi_t, p_t) = -\\pi_t p_t + \\lambda \\pi_t^2 + \\phi \\left( \\pi_t -\n",
    "\\pi_{t-1}\\right)^2;\\hspace{2mm} \\lambda, \\phi > 0\n",
    "$$\n",
    "\n",
    "Given the predictor signal $\\left( p_t\\right)_t$ the optimal position $\\pi_t$ at time\n",
    "$t$ has the following form\n",
    "$$\n",
    "\\pi_t = b\\times EMA_a\\left( p, t\\right)\n",
    "$$\n",
    "Where $EMA_a\\left( p, t\\right)$ denotes the exponential moving average of preditor\n",
    "signal $p$ at time $t$ with a decay of $0 < a \\le 1$ and $b > 0$\n",
    "\n",
    "We can now perform a grid-search over parameters $a$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Environment***\n",
    "\n",
    "We set our environment with the follwing parameters:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\theta = 0.1 \\\\\n",
    "\\sigma = 0.1 \\\\\n",
    "T = 5000 \\\\\n",
    "\\lambda = 0.3 \\\\\n",
    "\\phi = 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "\n",
    "SIGMA = 0.1\n",
    "THETA = 0.1\n",
    "T = 5000\n",
    "LAMBD = 0.3\n",
    "PSI = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(sigma=SIGMA, theta=THETA, T=T, lambd=LAMBD,\n",
    "                  psi=PSI, cost='trade_l2', scale_reward=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent parameters\n",
    "MAX_STEPS = 5000\n",
    "MEMORY_TYPE = 'prioritized'\n",
    "SLIDING = 'oldest'\n",
    "BATCH_SIZE = 2**9\n",
    "MAX_SIZE = int(1e6)\n",
    "\n",
    "# Training parameters\n",
    "TOTAL_EPISODES = 200  # set it to 501 for better convergence\n",
    "TOTAL_STEPS = 10000\n",
    "FREQ = 10\n",
    "LEARN_FREQ = 50\n",
    "TAU_ACTOR = 0.3\n",
    "TAU_CRITIC = 0.1\n",
    "LR_ACTOR = 1e-3\n",
    "LR_CRITIC = 1e-2\n",
    "WEIGHTS_DECAY_ACTOR = 0\n",
    "WEIGHTS_DECAY_CRITIC = 0\n",
    "FC1_UNITS_ACTOR = 16\n",
    "FC2_UNITS_ACTOR = 16\n",
    "FC1_UNITS_CRITIC = 16\n",
    "FC2_UNITS_CRITIC = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = 'Experiment_squared_cost/'\n",
    "if not os.path.exists(path + 'weights/'):\n",
    "    os.makedirs(path+'weights/')\n",
    "\n",
    "agent = Agent(max_size=MAX_SIZE, max_step=MAX_STEPS, memory_type=MEMORY_TYPE,\n",
    "              sliding=SLIDING, batch_size=BATCH_SIZE, alpha=0.6, theta=.1)\n",
    "# agent = Agent(max_size = MAX_SIZE, max_step=MAX_STEPS, memory_type=MEMORY_TYPE,\n",
    "# sliding=SLIDING, batch_size=BATCH_SIZE, alpha=0.6, point_max=2, n_points=5)\n",
    "agent.train(env=env, total_episodes=TOTAL_EPISODES, tau_actor=TAU_ACTOR,\n",
    "            tau_critic=TAU_CRITIC, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC,\n",
    "            weight_decay_actor=WEIGHTS_DECAY_ACTOR,\n",
    "            weight_decay_critic=WEIGHTS_DECAY_CRITIC, total_steps=TOTAL_STEPS,\n",
    "            weights=path+'weights/', freq=FREQ, fc1_units_actor=FC1_UNITS_ACTOR,\n",
    "            fc2_units_actor=FC2_UNITS_ACTOR, fc1_units_critic=FC1_UNITS_CRITIC,\n",
    "            fc2_units_critic=FC2_UNITS_CRITIC, learn_freq=LEARN_FREQ, plots=True,\n",
    "            lambd=.33, psi=.47, tensordir=path+'runs/', mile=100, decay_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1024\n",
    "n_episodes = 10\n",
    "rng = np.random.RandomState(random_state)\n",
    "random_states = rng.randint(0, int(1e6), size=n_episodes)\n",
    "score, score_episode, scores_cumsum_opt, pnls_opt, positions_opt = env.test_apply(\n",
    "    total_episodes=n_episodes, random_states=random_states, lambd=.33, psi=.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_weights = path+'weights/'\n",
    "scores, scores_episodes, scores_cumsum, pnls, positions = test_models(\n",
    "    path_weights, env, n_episodes=10, fc1_units=16, fc2_units=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1024\n",
    "n_episodes = 10\n",
    "rng = np.random.RandomState(random_state)\n",
    "random_states = rng.randint(0, int(1e6), size=n_episodes)\n",
    "score, score_episode, scores_cumsum_opt, pnls_opt, positions_opt = env.test_apply(\n",
    "    total_episodes=n_episodes, random_states=random_states, thresh=.95)\n",
    "scores[-1] = score\n",
    "scores_episodes[-1] = score_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_keys = [0, 40, 60, 100, 120]\n",
    "plot_function(path_weights, env, models_keys, low=-4, high=4, lambd=0.3,\n",
    "              fc1_units=16, fc2_units=16, thresh=.95, clip=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.3"
   }
  },
  "kernelspec": {
   "display_name": "deep_rl_for_portfolio_optimization",
   "language": "python",
   "name": "deep_rl_for_portfolio_optimization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
